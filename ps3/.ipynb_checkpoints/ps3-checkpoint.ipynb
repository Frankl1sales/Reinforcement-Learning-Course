{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656f8afe-d315-4948-9c9e-6b7da5773761",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9175826fd75c114e00bf025ad5dfc3da",
     "grade": false,
     "grade_id": "cell-4c19c1d2c77c4758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lista de Exercícios 3: Aproximação de Funções\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c2dd1-29c7-4dd2-9b4b-3780cc7a1136",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cc7d6658f61e5d6da8dbcc48df24048",
     "grade": false,
     "grade_id": "cell-f9d9bd24b843d103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8cafad-b798-43c4-acf0-319b049f9518",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "577070e8ade874cd808f2d0374ff6b5c",
     "grade": false,
     "grade_id": "cell-f164f0410942aeb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- ***SUBMISSÕES QUE NÃO SEGUIREM AS INSTRUÇÕES A SEGUIR NÃO SERÃO AVALIADAS.***\n",
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido antes de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps3.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 30/10/2025. Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa68b5-b720-41fc-b6e2-aa8a3f899aaa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "070293a3324a7cc123e3fb82eda16153",
     "grade": false,
     "grade_id": "cell-f9a3ddde9dedcf66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Mountain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f984dec9-c68b-4c81-9b84-4a04342618b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "074ed009b6ecb9654239d5c90e221810",
     "grade": false,
     "grade_id": "cell-64a3af160a59fce5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mountain Car é um ambiente no qual um agente precisa conduzir um carro até o topo de uma montanha. No entanto, o motor do carro não é potente o suficiente para subir diretamente até o topo. Por isso, o agente precisa aprender a aproveitar o impulso - movendo-se para frente e para trás - para ganhar velocidade suficiente e alcançar o topo.\n",
    "\n",
    "![](https://gymnasium.farama.org/_images/mountain_car.gif)\n",
    "\n",
    "O espaço de observação é contínuo e composto por dois valores: a posição e a velocidade do carro. O agente pode escolher entre três ações discretas: acelerar o carro para a esquerda, acelerar para a direita, ou não acelerar. A posição inicial do agente é definida de forma uniformemente aleatória no intervalo $[-0.6, -0.4]$. O episódio se encerra quando o carro atinge o topo da montanha à direita (posição $0.5$) ou quando o limite de 200 passos é atingido. A cada passo, o agente recebe uma penalidade de $-1$, incentivando-o a alcançar o objetivo no menor número possível de passos. Para mais detalhes sobre o ambiente leia a [documentação do gymnasium](https://gymnasium.farama.org/environments/classic_control/mountain_car/).\n",
    "\n",
    "Devido à natureza contínua do espaço de estados, métodos tabulares não são eficazes no Mountain Car. Assim, é comum o uso de técnicas de aproximação de função para solucionar o problema de forma eficiente. Nesta lista de exercícios, você irá trabalhar com o ambiente descrito acima. Seu objetivo será implementar o algoritmo *Semi-Gradient Episodic Sarsa* com aproximação linear e explorar como diferentes técnicas de construção de features influenciam o desempenho do agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530b8ce-4fd5-4394-bd10-e609ba76f987",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2622ec1bcee08e1245fc15de2ae6d243",
     "grade": false,
     "grade_id": "cell-123718121e8a49d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Semi-Gradient Episodic Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c78de-e691-4da5-a116-0b880e4e4ba7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f109362cd1f2d0a8aee8bc61ed2ee7ed",
     "grade": false,
     "grade_id": "cell-e469841ad084257f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nesta atividade, você implementará um agente baseado no algoritmo Semi-Gradient Episodic SARSA, utilizando uma **função de valor linear**.\n",
    "\n",
    "Antes de iniciar sua implementação, analise a interface `FeatureExtractor` fornecida abaixo. Ela será a base para os construtores de features que você implementará na próxima seção. O agente utilizará um objeto com essa interface para extrair features a partir das observações dos estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8031cdc8-e000-4924-afd4-3c629d9ad514",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00cf4f6ad83cd5e1d7f381d01308234b",
     "grade": false,
     "grade_id": "cell-3acae5684f619fa4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a49285f-de9f-4875-bc7c-320c608dd5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d17f85efd8fa3151d16dcba1aa33344",
     "grade": false,
     "grade_id": "cell-73b0cc412a521e00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(ABC):\n",
    "    \"\"\"\n",
    "    Interface for feature extractors that convert environment states into feature vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Property that returns the size of the feature vector produced by this extractor.\n",
    "\n",
    "        Returns:\n",
    "            An integer representing the length of the feature vector.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self, state):\n",
    "        \"\"\"\n",
    "        Converts a raw state into a 1D feature vector.\n",
    "\n",
    "        Args:\n",
    "            state: The observation state from the environment.\n",
    "\n",
    "        Returns:\n",
    "            The extracted feature vector representation.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd9099-4e4b-4a64-9523-06000e00dcbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b6ba7d77909531d7cc02f88ee551c2b",
     "grade": false,
     "grade_id": "cell-9bcee510ee1c562f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Siga as instruções abaixo para implementar seu agente:\n",
    "\n",
    "1. Implemente o método `__init__` que inicializa um novo agente. Ele deve receber como parâmetros o espaço de observações, o espaço de ações, um construtor de features (`feature_extractor`), a taxa de aprendizado $\\alpha$, o fator de desconto $\\gamma$, e o parâmetro de exploração $\\varepsilon$. Inicialize os pesos do modelo na variável `self.weights`.\n",
    "2. Implemente o método `compute_q_values`, que recebe um vetor de features e calcula os *Q-values* de acordo com a entrada.\n",
    "3. Implemente o método `choose_action`, responsável por escolher uma ação a partir de um estado observado, seguindo a política $\\varepsilon$-greedy.\n",
    "4. Implemente o método `learn`, que atualiza os pesos do agente com base na experiência obtida durante a interação com o ambiente.\n",
    "5. Implemente o método `train`, que executa o loop de treinamento do algoritmo Sarsa. O ambiente de treinamento e o número de episódios devem ser fornecidos como parâmetros de entrada. O método deve retornar uma lista com a soma das recompensas obtidas ao longo de cada episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879de152-526a-4a56-b28c-a725d3ae5524",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f706253d52ac575b9e8aa0b3181f3fc",
     "grade": false,
     "grade_id": "cell-5a69b1dafa706f11",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SGESAgent:\n",
    "    def __init__(self, observation_space, action_space, feature_extractor, alpha, gamma, epsilon):\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        self.action_space = action_space\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.alpha = alpha  # Taxa de aprendizado\n",
    "        self.gamma = gamma  # Fator de desconto\n",
    "        self.epsilon = epsilon  # Parâmetro de exploração \n",
    "        \n",
    "        # Inicializa os pesos\n",
    "        self.weights = np.zeros((self.feature_extractor.feature_size, self.action_space.n))\n",
    "        \n",
    "        # Gerador de n aleatorios\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def compute_q_values(self, features):\n",
    "        #Calcula os Q-values para o vetor de features\n",
    "        #Q(s, a) = w_a^T * x(s)\n",
    "        \n",
    "        # Multiplicação de matriz: (1, feature_size) . (feature_size, num_actions) -> (1, num_actions)\n",
    "        return np.dot(features, self.weights)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação usando a política epsilon-greedy.\n",
    "        \"\"\"\n",
    "        # Exploração: escolhe uma ação aleatória\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        \n",
    "        # Explotação: escolhe a melhor ação\n",
    "        else:\n",
    "            features = self.feature_extractor.extract(state)\n",
    "            q_values = self.compute_q_values(features)\n",
    "            # Retorna a ação com o maior Q-value\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"\n",
    "        Atualiza os pesos do modelo usando a regra de atualização do SARSA semi-gradiente.\n",
    "        \"\"\"\n",
    "        # Extrai features para o estado atual e o próximo estado\n",
    "        features = self.feature_extractor.extract(state)\n",
    "        next_features = self.feature_extractor.extract(next_state)\n",
    "        \n",
    "        # Calcula Q(s, a)\n",
    "        q_value = self.compute_q_values(features)[action]\n",
    "        \n",
    "        # Calcula Q(s', a')\n",
    "        q_next = 0.0\n",
    "        if not done:\n",
    "            q_next = self.compute_q_values(next_features)[next_action]\n",
    "            \n",
    "        # Calcula o alvo (target) da atualização (R + gamma * Q(s', a'))\n",
    "        target = reward + self.gamma * q_next\n",
    "        \n",
    "        # Calcula o erro TD (delta)\n",
    "        delta = target - q_value\n",
    "        \n",
    "        # Atualiza os pesos para a AÇÃO ESPECÍFICA\n",
    "        # w_a <- w_a + alpha * delta * x(s)\n",
    "        self.weights[:, action] += self.alpha * delta * features\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        \"\"\"\n",
    "        Executa o loop de treinamento do agente.\n",
    "        \"\"\"\n",
    "        episode_returns = []  # Lista para armazenar os retornos de cada episódio\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            total_reward = 0\n",
    "            state, info = env.reset()\n",
    "            \n",
    "            # Escolhe a primeira ação (A)\n",
    "            action = self.choose_action(state)\n",
    "            \n",
    "            done = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not done and not truncated:\n",
    "                # Executa a ação (A), observa R, S'\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Escolhe a próxima ação (A')\n",
    "                next_action = self.choose_action(next_state)\n",
    "                \n",
    "                # Atualiza os pesos usando (S, A, R, S', A')\n",
    "                self.learn(state, action, reward, next_state, next_action, done)\n",
    "                \n",
    "                # S <- S', A <- A'\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                \n",
    "            episode_returns.append(total_reward)\n",
    "            \n",
    "        return episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d770e7c-d0f0-40ac-8542-be19bb633788",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dce024544d538817499b0a09ca891f4c",
     "grade": true,
     "grade_id": "cell-1581dad1471d2254",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a880af-b6d9-49c7-b2ca-e608aa59227d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f13b804c440019ba980f44df55f8f08a",
     "grade": true,
     "grade_id": "cell-d9ad11a09933e17c",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1e0efc-d69d-453b-aade-c1ae507104e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31d43906c48c7039240e1fea9af4f6c1",
     "grade": true,
     "grade_id": "cell-6bbe66dac39d7e17",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabe66ff-2d3e-471e-bc81-7970a5b9bb0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f2ec3f3833b047e8971d71adf1c2e57",
     "grade": true,
     "grade_id": "cell-fac233e6d8645e9e",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0495d16-1298-4cd3-9193-dccc149c4678",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe717c67b91fd9975f181dcd48a43153",
     "grade": false,
     "grade_id": "cell-e42599779f53b3e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Construção de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61beab6-4a29-4505-b8a5-cdaec5484cf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2646c9d4329584c597ff4d3371f399d8",
     "grade": false,
     "grade_id": "cell-d264a9f794c5e744",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Com o algoritmo do agente pronto, você deverá implementar 3 modelos de construtores de features diferentes:\n",
    "\n",
    "1. **Identidade**\n",
    "2. **Tile Coding**\n",
    "3. **Funções de Base Radial**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb71073-f8ef-462b-a25f-5ec7b1a2f419",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2a6122e5b5a7d4ab935c66cf4b5d5f9",
     "grade": false,
     "grade_id": "cell-08d1055c0c0a1318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Identidade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278df1d7-d78d-46d1-aaab-a35613e94e6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d3865f8f2d1e6fb59386471a9b740e5",
     "grade": false,
     "grade_id": "cell-54e7e236f108e69a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Implemente o `IdentityFeatureExtractor`, cujo método `extract` apenas retorna a observação de entrada, sem realizar nenhuma operação nela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15a85dc-5a11-42f8-bdab-5c261cd71d18",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a61b75778c03707f4724dca465b40c7",
     "grade": false,
     "grade_id": "cell-8c81c63bcd766788",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, observation_space):\n",
    "        # Armazena o espaço de observação para consulta futura\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        # O tamanho da feature é o número de dimensões do estado (2 para MountainCar)\n",
    "        return self.observation_space.shape[0]\n",
    "        \n",
    "    def extract(self, state):\n",
    "        # Retorna o estado como está\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1a520b-9eb6-4d8e-b2e2-cfec25b396bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6296588cdfbd3989801ea511d8a7d58e",
     "grade": true,
     "grade_id": "cell-7b19ca6e8a004b3e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de609b7-083d-4af7-a727-d57e438b3722",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "725009ceae186f4b5b769c8e129e7e7f",
     "grade": false,
     "grade_id": "cell-fdf844a8a866e21c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. Treine um novo agente no ambiente Mountain Car por 200 episódios utilizando o `IdentityFeatureExtractor`. O agente deve ser treinado com os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.01$, fator de desconto $\\gamma = 0.99$ e parâmetro de exploração $\\varepsilon = 0.1$. Armazene os retornos episódicos, obtidos no método `train`, na variável `identity_agent`.\n",
    "\n",
    "**Nota:** Não se preocupe se o desempenho do agente com o `IdentityFeatureExtractor` for insatisfatório. Ele será utilizado apenas como baseline, permitindo observar o comportamento do agente quando nenhuma transformação é aplicada às observações do ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ff2b1f-bbbc-4ce3-9f35-14d2ebbc9dd3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bac7fe0ba1b8b2af3f80a38646d2543",
     "grade": false,
     "grade_id": "cell-085f6d93aa847917",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m SGESAgent(env\u001b[38;5;241m.\u001b[39mobservation_space, \n\u001b[1;32m      9\u001b[0m                   env\u001b[38;5;241m.\u001b[39maction_space, \n\u001b[1;32m     10\u001b[0m                   identity_extractor, \n\u001b[1;32m     11\u001b[0m                   alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \n\u001b[1;32m     12\u001b[0m                   gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, \n\u001b[1;32m     13\u001b[0m                   epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Treinar o agente\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m identity_agent \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     20\u001b[0m mean_return \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(identity_agent)\n",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m, in \u001b[0;36mSGESAgent.train\u001b[0;34m(self, env, episodes)\u001b[0m\n\u001b[1;32m     66\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Escolhe a primeira ação (A)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     72\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mSGESAgent.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mCalcula os Q-values para um dado vetor de features.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mQ(s, a) = w_a^T * x(s)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Multiplicação de matriz: (1, feature_size) . (feature_size, num_actions) -> (1, num_actions)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[43mfeatures\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# 1. Criar o extrator de features\n",
    "identity_extractor = IdentityFeatureExtractor(env.observation_space)\n",
    "\n",
    "# 2. Criar o agente\n",
    "# Parâmetros conforme especificado: alpha=0.01, gamma=0.99, epsilon=0.1\n",
    "agent = SGESAgent(env.observation_space, \n",
    "                  env.action_space, \n",
    "                  identity_extractor, \n",
    "                  alpha=0.01, \n",
    "                  gamma=0.99, \n",
    "                  epsilon=0.1)\n",
    "\n",
    "# 3. Treinar o agente\n",
    "identity_agent = agent.train(env, 200)\n",
    "\n",
    "env.close()\n",
    "\n",
    "mean_return = np.mean(identity_agent)\n",
    "print(f\"Mean Return: {mean_return:.2f}\")\n",
    "\n",
    "last_mean = np.mean(identity_agent[-20:])\n",
    "print(f\"Mean Return of the Last 20 Episodes: {last_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ede4a-ddcb-408b-b405-2d19378947cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aea3f40a80395ea1a24d2f4c8714a3c",
     "grade": true,
     "grade_id": "cell-d1981b644a5b113b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f23c80-882a-4102-9602-362d9d9bfae4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76621f30b1d49456eb3d201455c196b6",
     "grade": false,
     "grade_id": "cell-c0bae56555c46531",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tile Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a676c2-7c62-459f-9312-fbbfdae074eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e48c2735b2cca94d407e83d349a9b21",
     "grade": false,
     "grade_id": "cell-250d747b0305c2b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. Implemente o `TileCodingFeatureExtractor`, cujo método `extract` gera um vetor de features utilizando a técnica de *tile coding*. A quantidade de tilings a ser gerada é definida pelo parâmetro `num_tilings`, enquanto a quantidade de divisões por dimensão em cada tiling é definida pelo parâmetro `tiles_per_dim`. Ambos valores são fornecidos ao construtor da classe. Por exemplo: um `TileCodingFeatureExtractor` com `num_tilings = 3` e `tiles_per_dim = 2` deve gerar 3 tilings, cada um dividindo o espaço em uma grade de tamanho 2$\\times$2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa3e95-c413-4145-8c95-706c8ffd62d9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34a8b68d2ef9f660410a88f95fae02bd",
     "grade": false,
     "grade_id": "cell-c5337854b951c2f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TileCodingFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, observation_space, num_tilings, tiles_per_dim):\n",
    "        self.observation_space = observation_space\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tiles_per_dim = tiles_per_dim\n",
    "        \n",
    "        self.num_dims = self.observation_space.shape[0]\n",
    "        \n",
    "        # Calcula o tamanho de cada \"tile\" (ladrilho)\n",
    "        self.tile_sizes = (self.observation_space.high - self.observation_space.low) / self.tiles_per_dim\n",
    "        \n",
    "        # Tamanho do vetor de features para um único tiling (ex: 8x8 = 64)\n",
    "        self.tiling_size = self.tiles_per_dim ** self.num_dims\n",
    "        \n",
    "        # Tamanho total do vetor de features (ex: 8 tilings * 64 features/tiling = 512)\n",
    "        self._feature_size = self.num_tilings * self.tiling_size\n",
    "        \n",
    "        # Calcula os deslocamentos (offsets) para cada tiling\n",
    "        # Cada tiling é deslocado por uma fração do tamanho do tile\n",
    "        self.offsets = np.array([i * self.tile_sizes / self.num_tilings for i in range(self.num_tilings)])\n",
    "\n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        return self._feature_size\n",
    "\n",
    "    def extract(self, state):\n",
    "        features = np.zeros(self.feature_size)\n",
    "        \n",
    "        for i in range(self.num_tilings):\n",
    "            # Aplica o deslocamento ao estado\n",
    "            offset_state = state + self.offsets[i]\n",
    "            \n",
    "            # Normaliza e discretiza o estado deslocado\n",
    "            scaled_state = (offset_state - self.observation_space.low) / self.tile_sizes\n",
    "            \n",
    "            # Converte para índices inteiros, garantindo que fiquem dentro dos limites [0, tiles_per_dim - 1]\n",
    "            tile_indices = np.clip(np.floor(scaled_state), 0, self.tiles_per_dim - 1).astype(int)\n",
    "            \n",
    "            # \"Achata\" os índices (ex: [x, y] -> x * N + y) para encontrar o índice dentro deste tiling\n",
    "            flat_index = 0\n",
    "            for dim in range(self.num_dims):\n",
    "                flat_index = flat_index * self.tiles_per_dim + tile_indices[dim]\n",
    "                \n",
    "            # Calcula o índice final no vetor de features completo\n",
    "            final_index = i * self.tiling_size + flat_index\n",
    "            \n",
    "            # Ativa a feature correspondente\n",
    "            features[final_index] = 1.0\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5718c6-1511-49e8-aa54-4e318a568752",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ed44b74cd58a3a950ece2ab94c78311",
     "grade": true,
     "grade_id": "cell-570f8900bc692806",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293b3c1-1f97-4736-b1c8-aa6db757df7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d5c4de8444919582b56764ec0d7219a",
     "grade": false,
     "grade_id": "cell-81ac171b1b0f806b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Treine um novo agente no ambiente Mountain Car por 200 episódios utilizando o `TileCodingFeatureExtractor`. O agente deve ser treinado com os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.01$, fator de desconto $\\gamma = 0.99$ e parâmetro de exploração $\\varepsilon = 0.1$. Para os parâmetros `num_tilings` e `tiles_per_dim`, utilize os valores que proporcionarem os melhores resultados. Armazene os retornos episódicos, obtidos no método `train`, na variável `tile_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd8ee1-eef8-4621-8664-c12a6cd8af12",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82a3486011a111511e8d473a390e8ef7",
     "grade": false,
     "grade_id": "cell-1c17a6bf39513cf8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Parâmetros de tiling (bons valores para este problema)\n",
    "num_tilings = 8\n",
    "tiles_per_dim = 8\n",
    "\n",
    "# 1. Criar o extrator de features\n",
    "tile_extractor = TileCodingFeatureExtractor(env.observation_space, \n",
    "                                            num_tilings=num_tilings, \n",
    "                                            tiles_per_dim=tiles_per_dim)\n",
    "\n",
    "# 2. Criar o agente\n",
    "# A taxa de aprendizado é sensível. Vamos usar a solicitada (0.01).\n",
    "# Para tile coding, é comum usar alpha / num_tilings.\n",
    "# Se o desempenho for ruim, pode ser necessário ajustar o alpha.\n",
    "# Vamos tentar: alpha = 0.01 (como pedido)\n",
    "# *Nota: Um alpha=0.1/num_tilings (ou seja, 0.0125) costuma ser mais rápido.\n",
    "# Vamos usar um alpha que é o alpha base (0.1) dividido pelo num_tilings\n",
    "# para seguir a boa prática, o que dá 0.0125, próximo do 0.01 pedido.\n",
    "# Vamos usar 0.0125.\n",
    "# ATUALIZAÇÃO: O prompt é explícito: \"alpha = 0.01\". Vamos usar 0.01.\n",
    "agent_alpha = 0.01 \n",
    "\n",
    "agent = SGESAgent(env.observation_space, \n",
    "                  env.action_space, \n",
    "                  tile_extractor, \n",
    "                  alpha=agent_alpha, \n",
    "                  gamma=0.99, \n",
    "                  epsilon=0.1)\n",
    "\n",
    "# 3. Treinar o agente\n",
    "tile_agent = agent.train(env, 200)\n",
    "\n",
    "env.close()\n",
    "\n",
    "mean_return = np.mean(tile_agent)\n",
    "print(f\"Mean Return: {mean_return:.2f}\")\n",
    "\n",
    "last_mean = np.mean(tile_agent[-20:])\n",
    "print(f\"Mean Return of the Last 20 Episodes: {last_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27900f37-5fb9-48b6-9f76-f9eee036ea07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e51a7cd6b30504fb0d52be1ef2de347f",
     "grade": true,
     "grade_id": "cell-4645209cd7a25a0e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8c52a-592e-4733-95c8-2ac0ccc37b9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d96586af9aee5787fc0dfc03d080b0e",
     "grade": false,
     "grade_id": "cell-698007219f41675d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Funções de Base Radial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d240d9-9de7-4a17-8555-5ba26b136ed1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e024714bdfe6542af3ce239c5a3642e",
     "grade": false,
     "grade_id": "cell-c24fd7907217afeb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. Implemente o `RBFFeatureExtractor`, cujo método `extract` gera um conjunto de features baseadas em *Radial Basis Functions*. A quantidade total de componentes a ser gerada é definida pelo parâmetro `n_components`, fornecido ao construtor da classe. Utilize a biblioteca *scikit-learn* (sklearn) para auxiliar sua implementação.\n",
    "\n",
    "**Importante:** Você pode combinar RBFs com diferentes parâmetros para capturar melhor dinâmicas complexas do ambiente e potencialmente melhorar o desempenho do agente. Experimente diferentes configurações para identificar as combinações que produzem os melhores resultados de aprendizado. Normalizar o vetor de entrada também pode facilitar o aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3308f7-d12d-457c-91fc-ba0608f14027",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80c20704e1481d4aa0f94726e1f8e20",
     "grade": false,
     "grade_id": "cell-e0d3eb3f4a41bb02",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "class RBFFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, observation_space, n_components):\n",
    "        self.observation_space = observation_space\n",
    "        self.n_components = n_components\n",
    "        \n",
    "        # Criamos um ambiente temporário para amostrar estados\n",
    "        env_temp = gym.make(\"MountainCar-v0\")\n",
    "        # Coleta 10000 amostras de estados para \"treinar\" o scaler e os samplers\n",
    "        sample_states = np.array([env_temp.observation_space.sample() for _ in range(10000)])\n",
    "        env_temp.close()\n",
    "        \n",
    "        # 1. Normalização\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(sample_states)\n",
    "        scaled_samples = self.scaler.transform(sample_states)\n",
    "        \n",
    "        # 2. Combinação de RBFs (como sugerido na dica)\n",
    "        # Vamos usar 4 gammas diferentes, com 1/4 dos componentes para cada\n",
    "        gammas = [0.5, 1.0, 2.0, 5.0]\n",
    "        n_per_sampler = self.n_components // len(gammas)\n",
    "        \n",
    "        self.samplers = []\n",
    "        for gamma in gammas:\n",
    "            sampler = RBFSampler(n_components=n_per_sampler, gamma=gamma)\n",
    "            sampler.fit(scaled_samples)\n",
    "            self.samplers.append(sampler)\n",
    "            \n",
    "        # O tamanho final da feature é a soma dos componentes de cada sampler\n",
    "        self._feature_size = n_per_sampler * len(gammas)\n",
    "\n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        return self._feature_size\n",
    "\n",
    "    def extract(self, state):\n",
    "        # O scaler espera uma entrada 2D (batch_size, num_features)\n",
    "        # Por isso, [state]\n",
    "        scaled_state = self.scaler.transform([state])\n",
    "        \n",
    "        # Extrai features de cada sampler\n",
    "        all_features = [s.transform(scaled_state) for s in self.samplers]\n",
    "        \n",
    "        # Concatena os resultados e achata para um vetor 1D\n",
    "        return np.concatenate(all_features, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a9d3f-5f7b-4afd-a95f-6f034683a870",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d4a84cf5ae7c7871fd0bb3d8dd1f96",
     "grade": true,
     "grade_id": "cell-ab38da8e2ed8b89b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b756f-5bd9-4fd8-8f17-b85397e776c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3e107c124cd1328ce19740ba33d6551",
     "grade": false,
     "grade_id": "cell-1a3aadd23d015a3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Treine um novo agente no ambiente Mountain Car por 200 episódios utilizando o `RBFFeatureExtractor` com 100 componentes. O agente deve ser treinado com os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.01$, fator de desconto $\\gamma = 0.99$ e parâmetro de exploração $\\varepsilon = 0.1$. Armazene os retornos episódicos, obtidos no método `train`, na variável `rbf_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566aed60-719d-49fa-bfb0-1d74c6148935",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b08f9ee642479890aabca12c0df6a86",
     "grade": false,
     "grade_id": "cell-bdafa3d25a88a2f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# 1. Criar o extrator de features RBF com 100 componentes\n",
    "n_components = 100\n",
    "rbf_extractor = RBFFeatureExtractor(env.observation_space, n_components=n_components)\n",
    "\n",
    "# 2. Criar o agente\n",
    "# Parâmetros conforme especificado: alpha=0.01, gamma=0.99, epsilon=0.1\n",
    "agent = SGESAgent(env.observation_space, \n",
    "                  env.action_space, \n",
    "                  rbf_extractor, \n",
    "                  alpha=0.01, \n",
    "                  gamma=0.99, \n",
    "                  epsilon=0.1)\n",
    "\n",
    "# 3. Treinar o agente\n",
    "rbf_agent = agent.train(env, 200)\n",
    "\n",
    "env.close()\n",
    "\n",
    "mean_return = np.mean(rbf_agent)\n",
    "print(f\"Mean Return: {mean_return:.2f}\")\n",
    "\n",
    "last_mean = np.mean(rbf_agent[-20:])\n",
    "print(f\"Mean Return of the Last 20 Episodes: {last_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42821725-3cf0-47f4-8fe4-aaea1cf59716",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e0e4a468e0cbd925631b2bc04ead2a5",
     "grade": true,
     "grade_id": "cell-aef7056df4c92e10",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'rbf_agent' in vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da40c2-2d34-48ea-bc7c-3462b1378677",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4ef875068b35e0ada7fc1cda52576c3",
     "grade": false,
     "grade_id": "cell-bcb7edcab7279bb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a471cf-0a42-4957-9da1-52561bb58199",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dfe64f9a9b817a3b821ccf830039af5",
     "grade": false,
     "grade_id": "cell-68c98e2523e84e43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Utilize a biblioteca matplotlib para construir um gráfico comparativo dos retornos episódicos obtidos pelos agentes utilizando cada um dos construtores de features implementados. Utilize as variáveis `identiy_agent`, `tile_agent` e `rbf_agent` obtidas nos exercícios anteriores. No eixo X, represente os episódios; no eixo Y, o retorno acumulado por episódio. Caso seja necessário, aplique uma média movel para suavizar as curvas e deixar as tendências mais evidentes. Inclua título, legendas e rótulos de eixos apropriados para facilitar a interpretação. Se utilizar algum tipo de suavização, indique claramente no gráfico qual o método aplicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90135676-0a9d-4b74-a424-0b627606f2de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66e658f6be2caabbc9ec8997ef82445f",
     "grade": true,
     "grade_id": "cell-6335198c450f6612",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define o tamanho da janela da média móvel para suavizar as curvas\n",
    "window_size = 10\n",
    "\n",
    "# Calcula a média móvel para cada agente\n",
    "identity_smooth = pd.Series(identity_agent).rolling(window_size, min_periods=1).mean()\n",
    "tile_smooth = pd.Series(tile_agent).rolling(window_size, min_periods=1).mean()\n",
    "rbf_smooth = pd.Series(rbf_agent).rolling(window_size, min_periods=1).mean()\n",
    "\n",
    "# Cria o gráfico\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(identity_smooth, label='Identity Extractor')\n",
    "plt.plot(tile_smooth, label='Tile Coding (8x8, 8 tilings)')\n",
    "plt.plot(rbf_smooth, label='RBF (100 componentes, 4 gammas)')\n",
    "\n",
    "# Adiciona títulos, rótulos e legenda\n",
    "plt.title(f'Desempenho de Extratores de Features no Mountain Car (Média Móvel de {window_size} episódios)')\n",
    "plt.xlabel('Episódio')\n",
    "plt.ylabel('Retorno Acumulado por Episódio')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9e281-2f0d-4204-9b1b-e5684feb0a5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06086dc71d78359622963096092f3663",
     "grade": false,
     "grade_id": "cell-2e00546e8b4c6fbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Explique por que a modelagem do construtor de features pode ser crucial para o desempenho de um agente que utiliza aproximação de função."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce766f7-d059-4c83-ae1d-e73585ee9df5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1c67a250750ccadbb7f49bf90d214f0",
     "grade": true,
     "grade_id": "cell-a5307d7b384c7373",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87757007-4320-4f83-9c36-50362b426f12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24e15393eb11f5e1d0d8bae7ef11bb17",
     "grade": false,
     "grade_id": "cell-f4ddf7228d082311",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "14. Quais critérios devem guiar a escolha dos modelos e da quantidade de features a serem utilizadas na construção do espaço de features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd436b1-067a-49c6-99e9-3e358e61239e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2afa89a168378d25b29fb861dabb9ae",
     "grade": true,
     "grade_id": "cell-5c2a50c39d4b19c5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
